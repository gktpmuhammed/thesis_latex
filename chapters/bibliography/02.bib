% ============================================================
% Chapter 2: Related Work â€” Bibliography
% ============================================================

% --- Vision Transformers ---

@inproceedings{dosovitskiy2021image,
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author    = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2021},
  url       = {https://arxiv.org/abs/2010.11929}
}

@inproceedings{he2022masked,
  title     = {Masked Autoencoders Are Scalable Vision Learners},
  author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {16000--16009},
  year      = {2022},
  url       = {https://arxiv.org/abs/2111.06377}
}

@article{chen2021transunet,
  title   = {TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation},
  author  = {Chen, Jieneng and Lu, Yongyi and Yu, Qihang and Luo, Xiangde and Adeli, Ehsan and Wang, Yan and Lu, Le and Yuille, Alan L. and Zhou, Yuyin},
  journal = {arXiv preprint arXiv:2102.04306},
  year    = {2021},
  url     = {https://arxiv.org/abs/2102.04306}
}

@inproceedings{hatamizadeh2022unetr,
  title     = {UNETR: Transformers for 3D Medical Image Segmentation},
  author    = {Hatamizadeh, Ali and Tang, Yucheng and Nath, Vishwesh and Yang, Dong and Myronenko, Andriy and Landman, Bennett and Roth, Holger R. and Xu, Daguang},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  pages     = {574--584},
  year      = {2022},
  url       = {https://arxiv.org/abs/2103.10504}
}

% --- Attention & Transformers ---

@inproceedings{vaswani2017attention,
  title     = {Attention Is All You Need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {30},
  year      = {2017},
  url       = {https://arxiv.org/abs/1706.03762}
}

% --- Vision-Language Models ---

@inproceedings{radford2021learning,
  title     = {Learning Transferable Visual Models from Natural Language Supervision},
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages     = {8748--8763},
  year      = {2021},
  publisher = {PMLR},
  url       = {https://arxiv.org/abs/2103.00020}
}

@inproceedings{li2022blip,
  title     = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  author    = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages     = {12888--12900},
  year      = {2022},
  publisher = {PMLR},
  url       = {https://arxiv.org/abs/2201.12086}
}

@article{li2023lavis,
  title   = {LAVIS: A One-stop Library for Language-Vision Intelligence},
  author  = {Li, Dongxu and Li, Junnan and Le, Hung and Wang, Guangsen and Savarese, Silvio and Hoi, Steven C.H.},
  journal = {arXiv preprint arXiv:2209.09019},
  year    = {2023},
  url     = {https://arxiv.org/abs/2209.09019}
}

@inproceedings{liu2024visual,
  title     = {Visual Instruction Tuning},
  author    = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {36},
  year      = {2024},
  url       = {https://arxiv.org/abs/2304.08485}
}

@article{alayrac2022flamingo,
  title   = {Flamingo: A Visual Language Model for Few-Shot Learning},
  author  = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and others},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume  = {35},
  pages   = {23716--23736},
  year    = {2022},
  url     = {https://arxiv.org/abs/2204.14198}
}

% --- Medical VLMs ---

@article{zhang2023biomedclip,
  title   = {BiomedCLIP: A Multimodal Biomedical Foundation Model Pretrained from Fifteen Million Scientific Image-Text Pairs},
  author  = {Zhang, Sheng and Xu, Yanbo and Usuyama, Naoto and Bagber, Jaspreet and Tinn, Robert and Preston, Sam and Rao, Rajesh and Wei, Mu and Valluri, Naveen and Teodoro, Cliff and others},
  journal = {arXiv preprint arXiv:2303.00915},
  year    = {2023},
  url     = {https://arxiv.org/abs/2303.00915}
}

@article{yang2024medgemma,
  title   = {MedGemma: Medical Gemma},
  author  = {Yang, {Google Health AI Team}},
  journal = {Google AI Blog},
  year    = {2024},
  url     = {https://ai.google.dev/gemma/docs/medgemma}
}

@article{team2024gemma,
  title   = {Gemma: Open Models Based on Gemini Research and Technology},
  author  = {{Gemma Team, Google DeepMind}},
  journal = {arXiv preprint arXiv:2403.08295},
  year    = {2024},
  url     = {https://arxiv.org/abs/2403.08295}
}

@misc{hamamci2024foundation,
  title         = {Generalist Foundation Models from a Multimodal Dataset for 3D Computed Tomography},
  author        = {Ibrahim Ethem Hamamci and Sezgin Er and Furkan Almas and Ayse Gulnihan Simsek and Sevval Nil Esirgun and Irem Dogan and Muhammed Furkan Dasdelen and Omer Faruk Durugol and Bastian Wittmann and Tamaz Amiranashvili and Enis Simsar and Mehmet Simsar and Emine Bensu Erdemir and Abdullah Alanbay and Anjany Sekuboyina and Berkan Lafci and Christian Bluethgen and Mehmet Kemal Ozdemir and Bjoern Menze},
  year          = {2024},
  eprint        = {2403.17834},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2403.17834}
}

% --- LLMs & Parameter-Efficient Fine-Tuning ---

@article{hu2022lora,
  title   = {LoRA: Low-Rank Adaptation of Large Language Models},
  author  = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal = {arXiv preprint arXiv:2106.09685},
  year    = {2022},
  url     = {https://arxiv.org/abs/2106.09685}
}

@article{dettmers2023qlora,
  title   = {QLoRA: Efficient Finetuning of Quantized Large Language Models},
  author  = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume  = {36},
  year    = {2023},
  url     = {https://arxiv.org/abs/2305.14314}
}

@article{touvron2023llama,
  title   = {LLaMA: Open and Efficient Foundation Language Models},
  author  = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal = {arXiv preprint arXiv:2302.13971},
  year    = {2023},
  url     = {https://arxiv.org/abs/2302.13971}
}

% --- Radiology Report Generation ---

@article{jing2018automatic,
  title   = {On the Automatic Generation of Medical Imaging Reports},
  author  = {Jing, Baoyu and Xie, Pengtao and Xing, Eric},
  journal = {arXiv preprint arXiv:1711.08195},
  year    = {2018},
  url     = {https://arxiv.org/abs/1711.08195}
}

@inproceedings{chen2020generating,
  title     = {Generating Radiology Reports via Memory-Driven Transformer},
  author    = {Chen, Zhihong and Song, Yan and Chang, Tsung-Hui and Wan, Xiang},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {1439--1449},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.16056}
}

@inproceedings{chen2022crossmodal,
  title     = {Cross-modal Memory Networks for Radiology Report Generation},
  author    = {Chen, Zhihong and Shen, Yaling and Song, Yan and Wan, Xiang},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2022},
  url       = {https://arxiv.org/abs/2204.13258}
}

@article{nicolson2023improving,
  title     = {Improving Chest X-Ray Report Generation by Leveraging Warm Starting},
  author    = {Nicolson, Aaron and Dowling, Jason and Kober, Bevan},
  journal   = {Artificial Intelligence in Medicine},
  volume    = {144},
  pages     = {102633},
  year      = {2023},
  publisher = {Elsevier},
  url       = {https://arxiv.org/abs/2201.09405}
}

@article{wang2023r2gengpt,
  title   = {R2GenGPT: Radiology Report Generation with Frozen LLMs},
  author  = {Wang, Zhanyu and Liu, Lingqiao and Wang, Lei and Zhou, Luping},
  journal = {Meta-Radiology},
  year    = {2023},
  url     = {https://arxiv.org/abs/2306.17477}
}

% --- 3D Medical Image Segmentation ---

@article{wasserthal2023totalsegmentator,
  title   = {TotalSegmentator: Robust segmentation of 104 anatomic structures in CT images},
  author  = {Wasserthal, Jakob and Breit, Hanns-Christian and Meyer, Manfred T. and Pradella, Maurice and Hinck, Daniel and Sauter, Alexander W. and Heye, Tobias and Boll, Daniel T. and Defined, Joshy and others},
  journal = {Radiology: Artificial Intelligence},
  volume  = {5},
  number  = {5},
  year    = {2023},
  url     = {https://arxiv.org/abs/2208.05868}
}

% --- Preprocessing / Data ---

@article{monai2020,
  title   = {Project MONAI},
  author  = {{MONAI Consortium}},
  journal = {Zenodo},
  year    = {2020},
  url     = {https://monai.io},
  doi     = {10.5281/zenodo.4323059}
}

@article{wolf2020transformers,
  title   = {Transformers: State-of-the-Art Natural Language Processing},
  author  = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and others},
  journal = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations},
  pages   = {38--45},
  year    = {2020},
  url     = {https://arxiv.org/abs/1910.03771}
}

% --- CNN-based classics (for historical context) ---

@inproceedings{he2016deep,
  title     = {Deep Residual Learning for Image Recognition},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {770--778},
  year      = {2016},
  url       = {https://arxiv.org/abs/1512.03385}
}

@article{lecun2015deep,
  title     = {Deep Learning},
  author    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal   = {Nature},
  volume    = {521},
  number    = {7553},
  pages     = {436--444},
  year      = {2015},
  publisher = {Nature Publishing Group},
  doi       = {10.1038/nature14539}
}

% --- Multi-task / Auxiliary Loss ---

@article{caruana1997multitask,
  title     = {Multitask Learning},
  author    = {Caruana, Rich},
  journal   = {Machine Learning},
  volume    = {28},
  number    = {1},
  pages     = {41--75},
  year      = {1997},
  publisher = {Springer},
  doi       = {10.1023/A:1007379606734}
}
