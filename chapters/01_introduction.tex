% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}
As deep learning has continued to advance over recent years, it has been applied to various fields of science and engineering. One of the most prominent applications of deep learning is in the field of computer vision. Deep learning models have achieved state-of-the-art performance on various computer vision tasks, such as image classification, object detection, and image segmentation.
However, deep learning models are data-hungry and require large amounts of labeled data to achieve high performance. This is especially true for medical imaging, where obtaining large-scale labeled datasets is challenging due to privacy concerns and the high cost of expert annotation. 
Traditional CNN-RNN-based frameworks suffered from a lack of localized reasoning and from weak integration of medical knowledge. This is partially solved by the introduction of the attention mechanism by transformers. 

Despite these improvements, several limitations still remain. Attention mechanisms improve the alignment between image regions and text tokens. However, most existing methods rely on 2D image representations. They process single images or independent slices from volumetric scans. As a result, they fail to capture full three-dimensional context. In clinical practice, many important findings depend on volumetric information, such as lesion shape, spatial extent, and inter-slice continuity. These details are often lost when using only 2D inputs. Therefore, models trained on 2D data struggle to perform consistent localized reasoning across multiple slices.

There are several reasons why most studies focus on 2D approaches. First, large-scale 3D medical datasets are difficult to collect and share. Privacy regulations and storage requirements make this process challenging. Second, training 3D deep learning models requires significantly higher computational resources. This limits their practical adoption. Third, most existing pretrained vision and vision–language models are designed for 2D data. As a result, transfer learning for 3D medical imaging is less mature. These factors have led researchers to favor simpler 2D pipelines, even though they do not fully represent clinical imaging data.

Recent advances in vision–language models (VLMs) offer new opportunities to overcome these limitations. VLMs enable stronger alignment between visual features and natural language. They support better reasoning across modalities. However, most medical VLM studies still rely on 2D image inputs. The potential of volumetric reasoning in combination with large language models remains underexplored.

This thesis aims to address this gap by focusing on 3D vision–language modeling for radiology report generation. A volumetric vision encoder is used to preserve spatial relationships across slices. This allows the model to better capture anatomical structure and disease patterns. In addition, state-of-the-art large language models are used to improve medical text understanding and generation. These models help produce clearer, more structured, and clinically accurate reports. By combining 3D visual reasoning with advanced medical language understanding, this work aims to build a more reliable and clinically meaningful report-generation system.
