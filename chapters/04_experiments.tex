\chapter{Experiments and Results}\label{chapter:experiments}
\section{Dataset}
We validate our methods using the \textbf{CT-RATE} dataset \cite{hamamci2024foundation}, a large-scale multimodal dataset for 3D computed tomography. CT-RATE is the first large-scale publicly available dataset that pairs full 3D medical volumes with free-text radiology reports, enabling the training of end-to-end 3D vision–language models for tasks such as report generation and text-conditional image synthesis \cite{hamamci2024generatect}.

\subsection{Dataset Overview}
The dataset consists of 25,692 3D chest CT volumes from 21,304 unique patients, paired with their corresponding free-text radiology reports. The data was collected from multiple sources and anonymized to ensure privacy. Key statistics of the dataset are summarized in Table \ref{tab:ctrate_stats} \cite{hamamci2024foundation}.

\begin{table}[htbp]
    \centering
    \caption{Key statistics of the CT-RATE dataset used in our experiments.}
    \label{tab:ctrate_stats}
    \begin{tabular}{ll}
        \toprule
        \textbf{Statistic} & \textbf{Value} \\ \midrule
        Total CT Volumes & 25,692 \\
        Unique Patients & 21,304 \\
        Modality & 3D Chest CT \\
        Text Modality & Radiology Reports (Findings \& Impression) \\ 
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Preprocessing}
Since CT volumes in the CT-RATE dataset vary significantly in all dimensions, we applied a standardized preprocessing pipeline to ensure a stable training input for our models. The steps, implemented using the MONAI library, are as follows:

\begin{enumerate}
    \item \textbf{Intensity Scaling}: We apply distinct intensity scaling to normalize Hounsfield Units (HU) to the range $[0, 1]$. We clip intensities between $-1150$ and $350$ HU, which covers the relevant radiodensity range for lung and soft tissue structures while suppressing extreme outliers.
    \item \textbf{Spatial Normalization}: To handle the variable volume sizes, we standardize the spatial dimensions to a fixed size of $112 \times 256 \times 352$ voxels. This is achieved by first applying \textit{Spatial Padding} with constant zero values to ensure the volume is at least the target size.
    \item \textbf{Dimension Reordering}: We ensure a channel-first data layout and transpose dimensions to match the input expectations of our 3D vision encoder.
\end{enumerate}

\begin{itemize}
    \item \textbf{Text Preprocessing}: To enable organ-level report generation, we employed an LLM-based decomposition strategy. We used a Large Language Model to decompose each full radiology report into organ-specific descriptions (e.g., "Lung", "Heart", "Liver"). This allows our model to learn fine-grained section-specific representations. We specifically filter for and utilize these decomposed sections, tokenizing them with a maximum sequence length of 128 tokens for our decoder.
\end{itemize}

Recent work has shown that using improved tokenization strategies can further enhance performance on this dataset \cite{hamamci2025bettertokensbetter3d}. We utilize the official train/validation/test splits provided by the authors to ensure comparable results with prior work.

\section{Evaluation Metrics}\label{sec:evaluation_metrics}
To assess the performance of our radiology report generation system we evaluate generated reports with multiple complementary metrics. Metrics are grouped by the type of signal they capture:

\begin{enumerate}
    \item \textbf{Surface-level (linguistic) metrics:} measure n-gram overlap and surface similarity to reference reports.
    \item \textbf{Semantic / embedding-based metrics:} measure semantic similarity using contextual embeddings.
    \item \textbf{Clinical / structured metrics:} evaluate clinical correctness by extracting findings, entities, relations or clinically-relevant labels.
    \item \textbf{Human evaluation and statistical tests:} measure radiologist preference, clinical usefulness, and statistical significance.
\end{enumerate}

\subsection{Surface-level (Natural Language Generation) Metrics}
These metrics are commonly used in image captioning and report generation benchmarks and capture lexical and syntactic similarity.

\subsubsection{BLEU}
BLEU \cite{papineni2002bleu} is an n-gram precision-based metric. For a candidate report $c$ and a set of references $\{r\}$, BLEU is computed as
\begin{equation}
    \text{BLEU} = \text{BP} \cdot \exp\!\left(\sum_{n=1}^{N} w_n \log p_n \right),
\end{equation}
where $p_n$ is the modified precision for $n$-grams, $w_n$ are weights (we use uniform weights $w_n = 1/N$), and BP is the brevity penalty:
\begin{equation}
    \text{BP} =
    \begin{cases}
        1 & \text{if } c_{len} > r_{len}^* \\
        \exp\left(1 - \frac{r_{len}^*}{c_{len}} \right) & \text{otherwise}
    \end{cases}
\end{equation}
Here $c_{len}$ is the candidate length and $r_{len}^*$ is the effective reference length (e.g., closest reference length). We report BLEU-4.

\subsubsection{ROUGE-L}
ROUGE-L \cite{lin2004rouge} uses the length of the Longest Common Subsequence (LCS) to capture sentence-level structure. Define
\[
R_{LCS} = \frac{\text{LCS}(c, r)}{|r|}, \qquad P_{LCS} = \frac{\text{LCS}(c, r)}{|c|},
\]
then the ROUGE-L F-measure is
\begin{equation}
F_{LCS} = \frac{(1+\beta^2) \, R_{LCS} \, P_{LCS}}{R_{LCS} + \beta^2 P_{LCS}},
\end{equation}
where $\beta$ weights recall relative to precision. In practice we set $\beta=1$ to report the balanced F$_1$ unless otherwise specified.

\subsubsection{METEOR}
METEOR \cite{banerjee2005meteor} aligns unigrams and computes a weighted harmonic mean of precision and recall that emphasizes recall. The core F$_{mean}$ is:
\begin{equation}
    F_{\text{mean}} = \frac{(1+\alpha) P R}{R + \alpha P},
\end{equation}
where typical METEOR uses $\alpha=9$ (equivalently $F_{\text{mean}} = \frac{10PR}{R + 9P}$). METEOR applies a fragmentation penalty:
\begin{equation}
    \text{Penalty} = 0.5 \left(\frac{\#\text{chunks}}{\#\text{matches}}\right)^3,
\end{equation}
and final score:
\begin{equation}
    \text{METEOR} = F_{\text{mean}} \cdot (1 - \text{Penalty}).
\end{equation}

\subsubsection{CIDEr}
CIDEr \cite{vedantam2015cider} uses TF–IDF weighting for n-grams and computes average cosine similarity between a candidate's and references' TF–IDF vectors. For n-grams of length $n$:
\begin{equation}
    \text{CIDEr}_n(c, r) = \frac{1}{M}\sum_{i=1}^{M}
        \frac{\mathbf{g}^n(c)\cdot \mathbf{g}^n(r_i)}
             {\|\mathbf{g}^n(c)\| \, \|\mathbf{g}^n(r_i)\|},
\end{equation}
and final CIDEr is the (optionally IDF-normalized) average across $n=1\ldots4$.

\vspace{2mm}
\textbf{Notes:} Surface metrics are sensitive to wording and may penalize clinically-correct paraphrases. Use them together with semantic and clinical metrics.

\subsection{Semantic / Embedding-based Metrics}
These metrics capture deeper (contextual) semantic similarity beyond n-gram overlap.

\subsubsection{BERTScore}
BERTScore computes pairwise token similarities using contextual token embeddings from a pre-trained transformer (e.g., BERT). For tokens in candidate and reference, token-wise cosine similarities are computed and aggregated into precision, recall and an $F_1$ score; optional IDF weighting can be applied \cite{zhang2019bertscore}.

\subsubsection{BERT-based contextual similarity}
Besides BERTScore, one can compute sentence-level embeddings (CLS / pooled embeddings) and cosine similarity, or use more specialized biomedical encoders (e.g., BioBERT, ClinicalBERT) to increase domain alignment.

\subsection{Clinical / Structured Metrics}
These metrics aim to measure clinical correctness and the presence/absence of findings.

\subsubsection{CheXbert}
CheXbert \cite{smit2020chexbert} is a BERT-based labeler trained on the output of the CheXpert rule-based labeler to extract findings from radiology reports with higher accuracy. We use it to detect the presence of 14 standard observations (e.g., Cardiomegaly, Edema, Pneumonia, No Finding). We compute the Micro-F$_1$ score across all 14 classes to measure the clinical accuracy of the generated reports compared to the ground truth.

\subsubsection{RadGraph}
RadGraph \cite{jain2021radgraph} converts radiology reports into knowledge graphs (entities and relations). Evaluation is done by measuring overlap between predicted and reference graphs; typical metrics are node (entity) Precision/Recall/F$_1$ and edge (relation) Precision/Recall/F$_1$. Reporting both entity-level and relation-level scores helps diagnose whether models miss or hallucinate links between findings and anatomy.

\subsubsection{SRR-BERT / Structured-report metrics}
SRR-BERT and similar methods fine-tune BERT-like encoders to score structured-report correctness; they are useful for fine-grained disease classification and structured-report components. Report the metric(s) as provided by the original implementation (precision/recall/F$_1$ or regression correlation).

\subsubsection{GREEN and other factuality metrics}
GREEN (Generative Radiology report Evaluation and Error Notation) and related metrics are designed to highlight factual errors and categorize them by severity (critical vs non-critical). Use them to count clinically-significant hallucinations and factual inconsistencies.

\subsubsection{Composite clinical metrics (e.g., RadCliQ)}
Composite metrics such as RadCliQ combine surface (e.g., BLEU) and clinical (e.g., CheXpert) signals, often with learned weights to better correlate with radiologist judgments. When using a learned composite, report how weights were obtained and validate correlation with human raters.

\section{Results}
\subsection{Quantitative Results}
Content goes here.
\subsection{Qualitative Results}
Content goes here.
